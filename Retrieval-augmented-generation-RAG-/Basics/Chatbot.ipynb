{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.55.1 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\n",
      "langchain-experimental 0.0.61 requires langchain-community<0.3.0,>=0.2.5, but you have langchain-community 0.0.20 which is incompatible.\n",
      "langchain-experimental 0.0.61 requires langchain-core<0.3.0,>=0.2.7, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-openai 0.1.13 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-openai 0.1.13 requires openai<2.0.0,>=1.32.0, but you have openai 1.6.1 which is incompatible.\n",
      "langchain-openai 0.1.13 requires tiktoken<1,>=0.7, but you have tiktoken 0.5.2 which is incompatible.\n",
      "langchain-text-splitters 0.2.1 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.1.23 which is incompatible.\n",
      "litellm 1.41.2 requires openai>=1.27.0, but you have openai 1.6.1 which is incompatible.\n",
      "litellm 1.41.2 requires tiktoken>=0.7.0, but you have tiktoken 0.5.2 which is incompatible.\n",
      "llama-index-agent-openai 0.2.7 requires openai>=1.14.0, but you have openai 1.6.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\thoma\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -qU \\\n",
    "    langchain==0.0.354 \\\n",
    "    openai==1.6.1 \\\n",
    "    datasets==2.10.1 \\\n",
    "    pinecone-client==3.1.0 \\\n",
    "    tiktoken==0.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the OpenAI Key from Enviornmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chat completion using Langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand about Entropy in Machine Learning. give latex code\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy in machine learning is a measure of randomness or disorder within a dataset. It is commonly used in decision trees to determine the best splitting criteria for creating branches that lead to more homogenous subsets of data. The formula for calculating entropy is given by:\n",
      "\n",
      "\\[ H(S) = -\\sum_{i=1}^{c} p_i \\log_{2}(p_i) \\]\n",
      "\n",
      "where:\n",
      "- \\( H(S) \\) is the entropy of the dataset \\( S \\),\n",
      "- \\( c \\) is the number of classes in the dataset,\n",
      "- \\( p_i \\) is the proportion of instances in class \\( i \\) in the dataset.\n",
      "\n",
      "Entropy reaches its maximum value when all classes are equally distributed and decreases as the classes become more homogenous. Decision tree algorithms like ID3 and C4.5 use entropy as a metric for selecting the best attribute to split on at each node.\n",
      "\n",
      "Let me know if you need more information on this topic.\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To give access to previous questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of physics, entropy is a measure of the disorder or randomness of a system. It is a thermodynamic quantity that characterizes the number of possible microscopic configurations that a system can have at a given macroscopic state. In physics, entropy is related to the second law of thermodynamics, which states that the total entropy of an isolated system always increases over time.\n",
      "\n",
      "In contrast, in machine learning, entropy is used as a measure of uncertainty or impurity in a data set. It is used to quantify the amount of disorder in the data and help make decisions in algorithms such as decision trees.\n",
      "\n",
      "So, while the concept of entropy in physics and machine learning both involve measures of disorder, they are used in different contexts and have different interpretations.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"How is it different from physics'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Information Manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLMChain within LangChain is the most common type of chain used for developing applications powered by language models. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. The purpose of the LLMChain is to take multiple input variables, format them using the PromptTemplate into a prompt, pass that prompt to the model (LLM or ChatModel), and then use the OutputParser (if provided) to parse the output of the model into a final format.\n",
      "\n",
      "The LangChain framework aims to enable applications that not only call out to a language model via an API but also connect the language model to other sources of data and allow the language model to interact with its environment. The design of LangChain is specifically focused on creating data-aware and agentic applications that leverage the power of language models in a more comprehensive and interactive way.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
